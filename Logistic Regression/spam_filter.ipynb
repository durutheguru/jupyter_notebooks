{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0                                                  1\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...\nNumber of spam messages: 747\nNumber of ham messages: 4825\n\n\n"
    }
   ],
   "source": [
    "%run ../include/util.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# /Users/dduru/PythonProjects/data/smsspamcollection/SMSSpamCollection\n",
    "df = read_csv_frame(delimiter='\\t', header=None)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print('Number of spam messages: %s' % df[df[0] == 'spam'][0].count())\n",
    "print('Number of ham messages: %s\\n\\n' % df[df[0] == 'ham'][0].count())\n",
    "\n",
    "X = df[1].values\n",
    "Y = df[0].values\n",
    "X_train_raw, X_test_raw, Y_train, Y_test = train_test_split(X, Y)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "# for i, prediction in enumerate(predictions[:15]):\n",
    "#     print('Predicted: %s, message: %s' % (prediction, X_test_raw[i]))\n",
    "\n",
    "# df = read_csv_frame(delimiter = 't', header = None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['boy', 'coming', 'is', 'me', 'rolling', 'see', 'to']\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "words_vec = vectorizer.fit_transform(['a boy is coming', 'a boy is coming to see me', 'see me rolling'])\n",
    "# print(words_vec)\n",
    "print(vectorizer.get_feature_names());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['go to the market', 'i went to the market yesterday', 'create a new thing so we can rest', 'a boy is coming quickly', 'he came to the house yesterday', 'we have to seek the right approach to marketing']\n"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    'Go to the Market',\n",
    "    'I went to the market yesterday',\n",
    "    'Create a new thing so we can rest',\n",
    "    'A boy is coming quickly',\n",
    "    'He came to the house yesterday',\n",
    "    'We have to seek the right approach to marketing'\n",
    "]\n",
    "\n",
    "lemmatized_sentences = []\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# for i, sentence in enumerate(sentences):\n",
    "#     words = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(sentence)]\n",
    "#     # words = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(sentence) if not word in set(stopwords.words('english'))]\n",
    "#     lemmatized_sentences.append(' '.join(words))\n",
    "\n",
    "\n",
    "[lemmatizer.lemmatize(word.lower()) for word in word_tokenize(sentences[3])]\n",
    "\n",
    "# print(lemmatized_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(lemmatized_sentences)\n",
    "\n",
    "x_test = vectorizer.transform(lemmatized_sentences_test)\n"
   ]
  },
  {
   "source": [
    "\n",
    "## Create Stacking Classifier ensemble\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10)),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "classifier = StackingClassifier(\n",
    "    estimators = estimators, \n",
    "    final_estimator=LogisticRegression()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}