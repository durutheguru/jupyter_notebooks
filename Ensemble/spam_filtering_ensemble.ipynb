{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600285342643",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Import nltk modules\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "source": [
    "\n",
    "### Initialize Word net lemmatizer and define utility functions\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    \n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "''\n",
    "def lemmatize(word):\n",
    "    pos_tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    pos_tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "\n",
    "    return lemmatizer.lemmatize(word.lower(), pos_tag_dict.get(pos_tag, wordnet.NOUN))\n",
    "\n",
    "\n",
    "def lemmatize_corpus(sentences):\n",
    "    lemmatized_sentences = []\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = [lemmatize(word) for word in word_tokenize(sentence)]\n",
    "        lemmatized_sentences.append(' '.join(words))\n",
    "\n",
    "    return lemmatized_sentences\n",
    "\n"
   ]
  },
  {
   "source": [
    "\n",
    "### Initialize Training and Test Data\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of spam messages: 747\nNumber of ham messages: 4825\n\n\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%run ../include/util.ipynb\n",
    "\n",
    "# /Users/dduru/PythonProjects/data/smsspamcollection/SMSSpamCollection\n",
    "df = read_csv_frame(delimiter='\\t', header=None)\n",
    "\n",
    "print('Number of spam messages: %s' % df[df[0] == 'spam'][0].count())\n",
    "print('Number of ham messages: %s\\n\\n' % df[df[0] == 'ham'][0].count())\n",
    "\n",
    "X = df[1].values\n",
    "Y = df[0].values\n",
    "\n",
    "train_sentences, test_sentences, train_verdicts, test_verticts = train_test_split(X, Y)\n",
    "\n",
    "\n",
    "# train_sentences = [\n",
    "#     'I am happy to see you today',\n",
    "#     'We were excited by the progress',\n",
    "#     'She could not contain her joy',\n",
    "#     'They were afraid of the clown',\n",
    "#     'She died in car crash last week',\n",
    "#     'We are not happy with the development',\n",
    "#     'Always optimistic about the future',\n",
    "#     'We want to do our best and succeed',\n",
    "#     'Competition broke out and hurt our business',\n",
    "#     'Keep working hard, things will work out',\n",
    "#     'Despite all the hard work, we failed',\n",
    "#     'We don\\'t know the way forward and have no plan to continue'\n",
    "# ]\n",
    "\n",
    "# test_sentences = [\n",
    "#     'Jolly good fellows came around today',\n",
    "#     'We don\\'t like the new arrange',\n",
    "#     'People are skeptical and refuse to commit',\n",
    "#     'Harry is confident in his chances of success',\n",
    "#     'Lola broke her neck and cannot continue'\n",
    "# ]\n",
    "\n",
    "# train_verdicts = [\n",
    "#     'positive',\n",
    "#     'positive',\n",
    "#     'positive',\n",
    "#     'negative',\n",
    "#     'negative', \n",
    "#     'negative', \n",
    "#     'positive',\n",
    "#     'positive',\n",
    "#     'negative',\n",
    "#     'positive',\n",
    "#     'negative',\n",
    "#     'negative'\n",
    "# ]\n",
    "\n",
    "# test_verticts = [\n",
    "#     'positive',\n",
    "#     'negative',\n",
    "#     'negative',\n",
    "#     'positive', \n",
    "#     'negative',\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "binarizer = LabelBinarizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "lemmatized_train_sentences = lemmatize_corpus(train_sentences)\n",
    "lemmatized_test_sentences = lemmatize_corpus(test_sentences)\n",
    "\n",
    "x_train = vectorizer.fit_transform(lemmatized_train_sentences)\n",
    "x_test = vectorizer.transform(lemmatized_test_sentences)\n",
    "\n",
    "y_train = binarizer.fit_transform(train_verdicts)\n",
    "y_test = binarizer.transform(test_verticts)\n",
    "\n",
    "\n",
    "# x_train = np.array(x_train)\n",
    "# y_train = np.array(y_train)\n",
    "# x_test = np.array(x_test)\n",
    "# y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9813352476669059\nLabel: spam\nText: Oh k...i'm watching here:)\nLabel: ham\nText: Hi, So, GPT-3 is all the buzz right now, isn’t it? Here’s what we’re doing about it: Recently, OpenAI announced GPT-3, their third language prediction model in the GPT-n series, considered by many to be the most advanced use of AI so far. TabNine, a Codota product, uses a variant of OpenAI’s GPT-2 model to provide incredible code completions. We are constantly working on improving our model to give you better and more powerful suggestions and enhance your productivity even further. Future improvements will bring the latest AI advancements right into your IDE. Therefore, we are super excited to witness a new advancement in the GPT-n series and its possible applications for TabNine.   In other words - stay tuned!\nLabel: ham\nText: Hello! click on this button for a chance to win one of many cash prizes. Hurry while stock lasts\nLabel: spam\nText: I miss you baby\nLabel: ham\nText: what are you doing now?\nLabel: ham\nText: enter into this lottery for a chance to win cash prizes\nLabel: spam\nText: You just won a cash prize of 500 naira\nLabel: spam\nText: win\nLabel: ham\nText: won\nLabel: spam\nText: winner\nLabel: ham\nText: winner won\nLabel: spam\nText: You will recieve your tone within the next 24hrs. For Terms and conditions please see Channel U\nLabel: ham\nText: To review and KEEP the fantastic Nokia N-Gage game deck with Club Nokia, go 2 www.cnupdates.com/newsletter. unsubscribe from alerts reply with the word OUT\nLabel: spam\nText: Had your mobile 10 mths? Update to the latest Camera/Video phones for FREE. KEEP UR SAME NUMBER, Get extra free mins/texts. Text YES for a call\nLabel: spam\nText: we are the champions\nLabel: ham\nText: -1\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "classifier = StackingClassifier(\n",
    "    estimators = estimators, \n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "classifier.fit(x_train.todense(), y_train)\n",
    "print(classifier.score(x_test.todense(), y_test))\n",
    "\n",
    "text = input('Enter text: ')\n",
    "while text != '-1':\n",
    "    lbl = binarizer.inverse_transform(\n",
    "        classifier.predict(vectorizer.transform(lemmatize_corpus([text])).todense())\n",
    "    )[0]\n",
    "    print(f'Label: {lbl}')\n",
    "    text = input('Enter text: ')\n",
    "    print(f'Text: {text}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}