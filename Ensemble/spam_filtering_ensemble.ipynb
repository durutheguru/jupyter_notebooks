{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600285342643",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Import nltk modules\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "source": [
    "\n",
    "### Initialize Word net lemmatizer and define utility functions\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    \n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "''\n",
    "def lemmatize(word):\n",
    "    pos_tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    pos_tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "\n",
    "    return lemmatizer.lemmatize(word.lower(), pos_tag_dict.get(pos_tag, wordnet.NOUN))\n",
    "\n",
    "\n",
    "def lemmatize_corpus(sentences):\n",
    "    lemmatized_sentences = []\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = [lemmatize(word) for word in word_tokenize(sentence)]\n",
    "        lemmatized_sentences.append(' '.join(words))\n",
    "\n",
    "    return lemmatized_sentences\n",
    "\n"
   ]
  },
  {
   "source": [
    "\n",
    "### Initialize Training and Test Data\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of spam messages: 747\nNumber of ham messages: 4825\n\n\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%run ../include/util.ipynb\n",
    "\n",
    "# /Users/dduru/PythonProjects/data/smsspamcollection/SMSSpamCollection\n",
    "df = read_csv_frame(delimiter='\\t', header=None)\n",
    "\n",
    "print('Number of spam messages: %s' % df[df[0] == 'spam'][0].count())\n",
    "print('Number of ham messages: %s\\n\\n' % df[df[0] == 'ham'][0].count())\n",
    "\n",
    "X = df[1].values\n",
    "Y = df[0].values\n",
    "\n",
    "train_sentences, test_sentences, train_verdicts, test_verticts = train_test_split(X, Y)\n",
    "\n",
    "\n",
    "# train_sentences = [\n",
    "#     'I am happy to see you today',\n",
    "#     'We were excited by the progress',\n",
    "#     'She could not contain her joy',\n",
    "#     'They were afraid of the clown',\n",
    "#     'She died in car crash last week',\n",
    "#     'We are not happy with the development',\n",
    "#     'Always optimistic about the future',\n",
    "#     'We want to do our best and succeed',\n",
    "#     'Competition broke out and hurt our business',\n",
    "#     'Keep working hard, things will work out',\n",
    "#     'Despite all the hard work, we failed',\n",
    "#     'We don\\'t know the way forward and have no plan to continue'\n",
    "# ]\n",
    "\n",
    "# test_sentences = [\n",
    "#     'Jolly good fellows came around today',\n",
    "#     'We don\\'t like the new arrange',\n",
    "#     'People are skeptical and refuse to commit',\n",
    "#     'Harry is confident in his chances of success',\n",
    "#     'Lola broke her neck and cannot continue'\n",
    "# ]\n",
    "\n",
    "# train_verdicts = [\n",
    "#     'positive',\n",
    "#     'positive',\n",
    "#     'positive',\n",
    "#     'negative',\n",
    "#     'negative', \n",
    "#     'negative', \n",
    "#     'positive',\n",
    "#     'positive',\n",
    "#     'negative',\n",
    "#     'positive',\n",
    "#     'negative',\n",
    "#     'negative'\n",
    "# ]\n",
    "\n",
    "# test_verticts = [\n",
    "#     'positive',\n",
    "#     'negative',\n",
    "#     'negative',\n",
    "#     'positive', \n",
    "#     'negative',\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "binarizer = LabelBinarizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "lemmatized_train_sentences = lemmatize_corpus(train_sentences)\n",
    "lemmatized_test_sentences = lemmatize_corpus(test_sentences)\n",
    "\n",
    "x_train = vectorizer.fit_transform(lemmatized_train_sentences)\n",
    "x_test = vectorizer.transform(lemmatized_test_sentences)\n",
    "\n",
    "y_train = binarizer.fit_transform(train_verdicts)\n",
    "y_test = binarizer.transform(test_verticts)\n",
    "\n",
    "\n",
    "# x_train = np.array(x_train)\n",
    "# y_train = np.array(y_train)\n",
    "# x_test = np.array(x_test)\n",
    "# y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9842067480258435\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=3)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "classifier = StackingClassifier(\n",
    "    estimators = estimators, \n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "classifier.fit(x_train.todense(), y_train)\n",
    "print(classifier.score(x_test.todense(), y_test))\n",
    "\n",
    "text = input('Enter text: ')\n",
    "while text != '-1':\n",
    "    lbl = binarizer.inverse_transform(\n",
    "        classifier.predict(vectorizer.transform(lemmatize_corpus([text])).todense())\n",
    "    )[0]\n",
    "    print(f'Label: {lbl}')\n",
    "    text = input('Enter text: ')\n",
    "    print(f'Text: {text}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}